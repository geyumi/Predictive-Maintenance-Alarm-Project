{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1r6ezP9y8h9RY4yTbKdh7_CXoqFI4u0_c","timestamp":1722512796120},{"file_id":"1E_IowjJu5BMGKzoiyNtv93k6XvZcXlDd","timestamp":1722511397735},{"file_id":"1CG2Ok_gURgOdpy2rGhNogGfMlC3AL7HA","timestamp":1722509828039},{"file_id":"1Stb44UNs65JToEb68RnV2DLRFcvpFAYZ","timestamp":1722507807037},{"file_id":"12LHom2unyMCifv5l61R5u4nDMZF0ys91","timestamp":1722506068318},{"file_id":"1rW_7S1eeEzqnSYFZm8ThbZUCLWclVF84","timestamp":1722496239768}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from google.colab import drive\n","from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n","from sklearn.cluster import KMeans\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Path of folder\n","folder_path = '/content/drive/MyDrive/Colab Notebooks/2024_08_01 for all sites kmeans site id wise last 3 months /'\n","\n","# Get list of all CSV files in the folder\n","csv_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.csv')]\n","\n","# Initialize an empty list to hold the dataframes\n","dfs = []\n","\n","# Loop through the CSV files and read each one into a dataframe\n","for file in csv_files:\n","    df = pd.read_csv(file)\n","    dfs.append(df)\n","\n","# Concatenate all dataframes into a single dataframe\n","data = pd.concat(dfs, ignore_index=True)\n","\n","# Sort by 'First Occurred On' in descending order (latest alarms first)\n","df = data.sort_values(by='First Occurred On', ascending=False)\n","\n","# Convert 'First Occurred On' to datetime, handle errors and fill NaT with a default date\n","df['First Occurred On'] = pd.to_datetime(df['First Occurred On'], errors='coerce')\n","\n","# Reference time\n","reference_time = pd.to_datetime('2024-08-01 22:00:00')\n","\n","# Calculate the relative day index, handle NaT by filling with a large negative number\n","df['Relative Day Index'] = ((df['First Occurred On'] - reference_time).dt.total_seconds() // 86400).fillna(-9999).astype(int)\n","\n","# Adjust the index to set the 08:00 to 08:00 of the next day as 0, and previous days as -1, -2, -3, etc.\n","df['Relative Day Index'] = df['Relative Day Index'].apply(lambda x: x if x < 0 else x)\n","\n","# Print the updated dataframe\n","print(df.head(50000))\n","\n","# Filter the data for HUAWEI and Access domain\n","filtered_data = df[(df['Vendor'] == 'HUAWEI')]\n","\n","# Retain only the specified columns\n","columns_to_keep = ['Site ID', 'Alarm Name', 'Vendor', 'Domain', 'Device Type', 'Relative Day Index']\n","df_filtered = filtered_data[columns_to_keep]\n","\n","# Identify the five most recent Site IDs\n","recent_site_ids = df_filtered['Site ID'].value_counts().index[:10]\n","\n","# Filter the dataframe to include only the most recent Site IDs\n","df_filtered = df_filtered[df_filtered['Site ID'].isin(recent_site_ids)]\n","\n","# Encode categorical columns except 'Relative Day Index'\n","categorical_columns = df_filtered.select_dtypes(include=['object']).columns\n","label_encoders = {}\n","for column in categorical_columns:\n","    le = LabelEncoder()\n","    df_filtered[column] = le.fit_transform(df_filtered[column].astype(str))\n","    label_encoders[column] = le\n","\n","# Print the updated dataframe\n","df_filtered.head(100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":581},"id":"KtMDgrMfKAEv","executionInfo":{"status":"error","timestamp":1724153013830,"user_tz":-330,"elapsed":71982,"user":{"displayName":"geyumi srisara","userId":"03202189726632949856"}},"outputId":"9cfba86f-aae4-43a7-e051-c9b76779b0c6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-1-1ab33db69822>:28: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(file)\n"]},{"output_type":"error","ename":"ParserError","evalue":"Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1ab33db69822>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Loop through the CSV files and read each one into a dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."]}]},{"cell_type":"code","source":["# Clustering to find common patterns\n","# Determine the optimal number of clusters using the elbow method\n","# wcss = []\n","# for i in range(1, 11):\n","#     kmeans = KMeans(n_clusters=i, random_state=42)\n","#     kmeans.fit(df_filtered)\n","#     wcss.append(kmeans.inertia_)\n","\n","# plt.plot(range(1, 11), wcss)\n","# plt.title('Elbow Method for Optimal k')\n","# plt.xlabel('Number of clusters')\n","# plt.ylabel('WCSS')\n","# plt.show()\n","\n","# Fit the KMeans model with the optimal number of clusters\n","optimal_clusters =20   # Increased number of clusters\n","kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n","df_filtered['Cluster'] = kmeans.fit_predict(df_filtered[['Alarm Name', 'Site ID']])\n","\n","# Now filter for individual site IDs after clustering\n","for site_id in recent_site_ids:\n","    past_alarms1 = df_filtered[df_filtered['Site ID'] == site_id]\n","\n","# Analyze the clusters\n","print(df_filtered.groupby('Cluster').mean())\n","print(df_filtered.groupby('Cluster').size())\n","\n","# Print cluster details and alarm patterns\n","cluster_patterns = {}\n","for cluster in range(optimal_clusters):\n","    print(f\"Cluster {cluster} details:\")\n","    cluster_data = df_filtered[df_filtered['Cluster'] == cluster]\n","    alarm_patterns = cluster_data.groupby(['Alarm Name', 'Site ID']).size()\n","    cluster_patterns[cluster] = alarm_patterns\n","    print(\"Alarm Patterns:\")\n","    print(alarm_patterns)\n","    print(\"\\n\")\n","\n","# Plot the clusters\n","# sns.pairplot(df_filtered, hue='Cluster')\n","# plt.show()\n","\n","# Inverse transform the scaled columns for interpretation\n","# scaler = MinMaxScaler()\n","# df_filtered[['Alarm Name', 'Site ID', 'Vendor', 'Domain', 'Device Type', 'Relative Day Index', 'Cluster']] = scaler.inverse_transform(df_filtered[['Alarm Name', 'Site ID', 'Vendor', 'Domain', 'Device Type', 'Relative Day Index', 'Cluster']])\n","\n","# Convert back to original categorical values using label_encoders\n","# for column in label_encoders.keys():\n","#     df_filtered[column] = label_encoders[column].inverse_transform(df_filtered[column].round().astype(int))\n","\n","# Display sample output of clustered data\n","df_filtered.head(100)"],"metadata":{"id":"ammOXKuasO3U","executionInfo":{"status":"aborted","timestamp":1724153013831,"user_tz":-330,"elapsed":73634,"user":{"displayName":"geyumi srisara","userId":"03202189726632949856"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Re-scale the dataframe for LSTM\n","scaler = MinMaxScaler()\n","df_filtered[['Alarm Name', 'Site ID', 'Vendor', 'Domain', 'Device Type', 'Relative Day Index', 'Cluster']] = scaler.fit_transform(df_filtered[['Alarm Name', 'Site ID', 'Vendor', 'Domain', 'Device Type', 'Relative Day Index', 'Cluster']])\n","\n","# Prepare the dataset for LSTM\n","def create_dataset(data, time_steps=1):\n","    X, y = [], []\n","    for i in range(len(data) - time_steps):\n","        X.append(data[i:(i + time_steps)])\n","        y.append(data[i + time_steps])\n","    return np.array(X), np.array(y)\n","\n","time_steps = 10\n","features = df_filtered[['Alarm Name', 'Site ID', 'Vendor', 'Domain', 'Device Type', 'Relative Day Index', 'Cluster']].values\n","X, y = create_dataset(features, time_steps)\n","\n","# Reshape input to be [samples, time steps, features]\n","X = X.reshape(X.shape[0], time_steps, X.shape[2])\n","\n","# Define the LSTM model\n","model = Sequential()\n","model.add(LSTM(50, return_sequences=True, input_shape=(time_steps, X.shape[2])))\n","model.add(LSTM(50))\n","model.add(Dense(X.shape[2]))\n","model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Train the model\n","model.fit(X, y, epochs=1, batch_size=2048, verbose=1)\n","\n","# Predict for the next 7 days\n","predictions = []\n","current_batch = features[-time_steps:].reshape((1, time_steps, X.shape[2]))\n","\n","for i in range(24 * 7):  # 24 hours * 7 days\n","    pred = model.predict(current_batch, verbose=0)[0]\n","    predictions.append(pred)\n","    current_batch = np.append(current_batch[:, 1:, :], [[pred]], axis=1)\n","\n","# Inverse transform the predictions\n","predictions = scaler.inverse_transform(predictions)\n","\n","# Create a DataFrame for predictions\n","relative_day_indices = np.arange(df_filtered['Relative Day Index'].max() + 1, df_filtered['Relative Day Index'].max() + 1 + 24 * 7)\n","pred_df = pd.DataFrame(predictions, columns=['Alarm Name', 'Site ID', 'Vendor', 'Domain', 'Device Type', 'Relative Day Index', 'Cluster'])\n","pred_df['Relative Day Index'] = relative_day_indices\n","\n","# Convert back to original categorical values using label_encoders\n","for column in label_encoders.keys():\n","    pred_df[column] = pred_df[column].round().astype(int)\n","    pred_df[column] = label_encoders[column].inverse_transform(pred_df[column])\n","\n","# Filter predictions for the next 7 days only\n","pred_df = pred_df[pred_df['Relative Day Index'] <= 7]\n","\n","# Display sample output of predictions with cluster details\n","print(pred_df.head(7))\n","\n","# Print the cluster patterns for the predicted alarms\n","for index, row in pred_df.iterrows():\n","    cluster = int(row['Cluster'])\n","    print(f\"Predicted Alarm for Relative Day Index {row['Relative Day Index']}:\")\n","    print(f\"Cluster {cluster} Alarm Patterns:\")\n","    alarm_names = cluster_patterns[cluster].index.get_level_values(0).map(lambda x: label_encoders['Alarm Name'].inverse_transform([x])[0])\n","    print(pd.Series(cluster_patterns[cluster].values, index=alarm_names))\n","    print(\"\\n\")\n","\n","# Print cluster-wise alarm patterns\n","for cluster in range(optimal_clusters):\n","    print(f\"Cluster {cluster} Alarm Patterns:\")\n","    alarm_names = cluster_patterns[cluster].index.get_level_values(0).map(lambda x: label_encoders['Alarm Name'].inverse_transform([x])[0])\n","    print(pd.Series(cluster_patterns[cluster].values, index=alarm_names))\n","    print(\"\\n\")"],"metadata":{"id":"sS72ELJpsTbQ","executionInfo":{"status":"aborted","timestamp":1724153013831,"user_tz":-330,"elapsed":73631,"user":{"displayName":"geyumi srisara","userId":"03202189726632949856"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Print cluster-wise alarm patterns\n","for cluster in range(optimal_clusters):\n","    print(f\"Cluster {cluster} Alarm Patterns:\")\n","    alarm_names = cluster_patterns[cluster].index.get_level_values(0).map(lambda x: label_encoders['Alarm Name'].inverse_transform([x])[0])\n","    alarm_counts = pd.Series(cluster_patterns[cluster].values, index=alarm_names)\n","    print(alarm_counts)\n","    print(\"\\n\")\n","\n","    # Plot the alarm patterns for the current cluster\n","    plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n","    alarm_counts.plot(kind='bar')\n","    plt.title(f'Alarm Patterns for Cluster {cluster}')\n","    plt.xlabel('Alarm Name')\n","    plt.ylabel('Frequency')\n","    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n","    plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n","    plt.show()"],"metadata":{"id":"ALxkmjDtmlky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Iterate through each site ID\n","for site_id in recent_site_ids:\n","\n","\n","    # Filter predicted alarms for the current site\n","    predicted_alarms = pred_df[pred_df['Site ID'] == site_id]\n","\n","    # Plot the alarms\n","    plt.figure(figsize=(12, 6))  # Adjust figure size as needed\n","\n","    # Plot past alarms\n","    plt.scatter(past_alarms1['Relative Day Index'], past_alarms1['Alarm Name'],\n","                c=past_alarms1['Cluster'], cmap='viridis', marker='o', label='Past Alarms', s=50)\n","\n","    # Plot predicted alarms\n","    plt.scatter(predicted_alarms['Relative Day Index'], predicted_alarms['Alarm Name'],\n","                c=predicted_alarms['Cluster'], cmap='viridis', marker='x', label='Predicted Alarms', s=50)\n","\n","    plt.title(f'Past and Predicted Alarms for Site {site_id}')\n","    plt.xlabel('Relative Day Index')\n","    plt.ylabel('Alarm Name')\n","    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels if needed\n","    plt.legend()\n","    plt.colorbar(label='Cluster')  # Add a colorbar to show cluster mapping\n","    plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n","    plt.show()"],"metadata":{"id":"k_QXGZ-LpxCp"},"execution_count":null,"outputs":[]}]}